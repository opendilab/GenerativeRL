{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Literal\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from easydict import EasyDict\n",
    "from functools import partial\n",
    "\n",
    "from edm_preconditioner import PreConditioner\n",
    "from edm_utils import SIGMA_T, SIGMA_T_DERIV, SIGMA_T_INV, SCALE_T, SCALE_T_DERIV, DEFAULT_SOLVER_PARAM\n",
    "from grl.generative_models.intrinsic_model import IntrinsicModel\n",
    "\n",
    "class Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "    def forward(self, x, noise, class_labels=None):\n",
    "        return self.model(x)\n",
    "\n",
    "class EDMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Optional[EasyDict]=None) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config= config\n",
    "        # self.x_size = config.x_size\n",
    "        self.device = config.device\n",
    "        \n",
    "        # EDM Type [\"VP_edm\", \"VE_edm\", \"iDDPM_edm\", \"EDM\"]\n",
    "        self.edm_type: str = config.edm_model.path.edm_type\n",
    "        assert self.edm_type in [\"VP_edm\", \"VE_edm\", \"iDDPM_edm\", \"EDM\"], \\\n",
    "            f\"Your edm type should in 'VP_edm', 'VE_edm', 'iDDPM_edm', 'EDM'], but got {self.edm_type}\"\n",
    "        \n",
    "        #* 1. Construct basic Unet architecture through params in config\n",
    "        self.base_denoise_network = Simple()\n",
    "\n",
    "        #* 2. Precond setup\n",
    "        self.params = config.edm_model.path.params\n",
    "        self.preconditioner = PreConditioner(\n",
    "            self.edm_type, \n",
    "            base_denoise_model=self.base_denoise_network, \n",
    "            use_mixes_precision=False,\n",
    "            **self.params\n",
    "        )\n",
    "        \n",
    "        #* 3. Solver setup\n",
    "        self.solver_type = config.edm_model.solver.solver_type\n",
    "        assert self.solver_type in ['euler', 'heun']\n",
    "        \n",
    "        self.solver_params = DEFAULT_SOLVER_PARAM\n",
    "        self.solver_params.update(config.edm_model.solver.params)\n",
    "        \n",
    "        # Initialize sigma_min and sigma_max if not provided\n",
    "        \n",
    "        if \"sigma_min\" not in self.params:\n",
    "            min = torch.tensor(1e-3)\n",
    "            self.sigma_min = {\n",
    "                \"VP_edm\": SIGMA_T[\"VP_edm\"](min, 19.9, 0.1), \n",
    "                \"VE_edm\": 0.02, \n",
    "                \"iDDPM_edm\": 0.002, \n",
    "                \"EDM\": 0.002\n",
    "            }[self.edm_type]\n",
    "        else:\n",
    "            self.sigma_min = self.params.sigma_min\n",
    "        if \"sigma_max\" not in self.params:\n",
    "            max = torch.tensor(1)\n",
    "            self.sigma_max = {\n",
    "                \"VP_edm\": SIGMA_T[\"VP_edm\"](max, 19.9, 0.1), \n",
    "                \"VE_edm\": 100, \n",
    "                \"iDDPM_edm\": 81, \n",
    "                \"EDM\": 80\n",
    "            }[self.edm_type]            \n",
    "        else:\n",
    "            self.sigma_max = self.params.sigma_max\n",
    "            \n",
    "    def get_type(self):\n",
    "        return \"EDMModel\"\n",
    "\n",
    "    # For VP_edm\n",
    "    def _sample_sigma_weight_train(self, x: Tensor, **params) -> Tuple[Tensor, Tensor]:\n",
    "        # assert the first dim of x is batch size\n",
    "        print(f\"params is {params}\")\n",
    "        rand_shape = [x.shape[0]] + [1] * (x.ndim - 1) \n",
    "        if self.edm_type == \"VP_edm\":\n",
    "            epsilon_t = params.get(\"epsilon_t\", 1e-5)\n",
    "            beta_d = params.get(\"beta_d\", 19.9)\n",
    "            beta_min = params.get(\"beta_min\", 0.1)\n",
    "            \n",
    "            rand_uniform = torch.rand(*rand_shape, device=x.device)\n",
    "            sigma = SIGMA_T[\"VP_edm\"](1 + rand_uniform * (epsilon_t - 1), beta_d, beta_min)\n",
    "            weight = 1 / sigma ** 2\n",
    "        elif self.edm_type == \"VE_edm\":\n",
    "            rand_uniform = torch.rand(*rand_shape, device=x.device)\n",
    "            sigma = self.sigma_min * ((self.sigma_max / self.sigma_min) ** rand_uniform)\n",
    "            weight = 1 / sigma ** 2\n",
    "        elif self.edm_type == \"EDM\":\n",
    "            P_mean = params.get(\"P_mean\", -1.2)\n",
    "            P_std = params.get(\"P_mean\", 1.2)\n",
    "            sigma_data = params.get(\"sigma_data\", 0.5)\n",
    "            \n",
    "            rand_normal = torch.randn(*rand_shape, device=x.device)\n",
    "            sigma = (rand_normal * P_std + P_mean).exp()\n",
    "            weight = (sigma ** 2 + sigma_data ** 2) / (sigma * sigma_data) ** 2\n",
    "        return sigma, weight\n",
    "    \n",
    "    def forward(self, \n",
    "                x: Tensor, \n",
    "                class_labels=None) -> Tensor:\n",
    "        x = x.to(self.device)\n",
    "        sigma, weight = self._sample_sigma_weight_train(x, **self.params)\n",
    "        n = torch.randn_like(x) * sigma\n",
    "        D_xn = self.preconditioner(x+n, sigma, class_labels=class_labels)\n",
    "        loss = weight * ((D_xn - x) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def _get_sigma_steps_t_steps(self, num_steps=18, epsilon_s=1e-3, rho=7):\n",
    "        \"\"\"\n",
    "        Overview:\n",
    "            Get the schedule of sigma according to differernt t schedules.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.sigma_min = max(self.sigma_min, self.preconditioner.sigma_min)\n",
    "        self.sigma_max = min(self.sigma_max, self.preconditioner.sigma_max)\n",
    "    \n",
    "        # Define time steps in terms of noise level\n",
    "        step_indices = torch.arange(num_steps, dtype=torch.float64, device=self.device)\n",
    "        sigma_steps = None\n",
    "        if self.edm_type == \"VP_edm\":\n",
    "            vp_beta_d = 2 * (np.log(self.sigma_min ** 2 + 1) / epsilon_s - np.log(self.sigma_max ** 2 + 1)) / (epsilon_s - 1)\n",
    "            vp_beta_min = np.log(self.sigma_max ** 2 + 1) - 0.5 * vp_beta_d\n",
    "            \n",
    "            orig_t_steps = 1 + step_indices / (num_steps - 1) * (epsilon_s - 1)\n",
    "            sigma_steps = SIGMA_T[\"VP_edm\"](orig_t_steps, vp_beta_d, vp_beta_min)\n",
    "        \n",
    "        elif self.edm_type == \"VE_edm\":\n",
    "            orig_t_steps = (self.sigma_max ** 2) * ((self.sigma_min ** 2 / self.sigma_max ** 2) ** (step_indices / (num_steps - 1)))\n",
    "            sigma_steps = SIGMA_T[\"VE_edm\"](orig_t_steps)\n",
    "        \n",
    "        elif self.edm_type == \"iDDPM_edm\":\n",
    "            M, C_1, C_2 = self.params.M, self.params.C_1, self.params.C_2\n",
    "            \n",
    "            u = torch.zeros(M + 1, dtype=torch.float64, device=self.device)\n",
    "            alpha_bar = lambda j: (0.5 * np.pi * j / M / (C_2 + 1)).sin() ** 2\n",
    "            for j in torch.arange(self.params.M, 0, -1, device=self.device): # M, ..., 1\n",
    "                u[j - 1] = ((u[j] ** 2 + 1) / (alpha_bar(j - 1) / alpha_bar(j)).clip(min=C_1) - 1).sqrt()\n",
    "            u_filtered = u[torch.logical_and(u >= self.sigma_min, u <= self.sigma_max)]\n",
    "            \n",
    "            sigma_steps = u_filtered[((len(u_filtered) - 1) / (num_steps - 1) * step_indices).round().to(torch.int64)]   \n",
    "            orig_t_steps = SIGMA_T_INV[self.edm_type](self.preconditioner.round_sigma(sigma_steps))         \n",
    "        \n",
    "        elif self.edm_type == \"EDM\": \n",
    "            sigma_steps = (self.sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * \\\n",
    "                (self.sigma_min ** (1 / rho) - self.sigma_max ** (1 / rho))) ** rho\n",
    "            orig_t_steps = SIGMA_T_INV[self.edm_type](self.preconditioner.round_sigma(sigma_steps))         \n",
    "        \n",
    "        t_steps = torch.cat([orig_t_steps, torch.zeros_like(orig_t_steps[:1])]) # t_N = 0\n",
    "        \n",
    "        return sigma_steps, t_steps  \n",
    "    \n",
    "    \n",
    "    def _get_sigma_deriv_inv_scale_deriv(self, epsilon_s=1e-3):\n",
    "        \"\"\"\n",
    "        Overview:\n",
    "            Get sigma(t) for different solver schedules.\n",
    "            \n",
    "        Returns:\n",
    "            sigma(t), sigma'(t), sigma^{-1}(sigma) \n",
    "        \"\"\"\n",
    "        vp_beta_d = 2 * (np.log(self.sigma_min ** 2 + 1) / epsilon_s - np.log(self.sigma_max ** 2 + 1)) / (epsilon_s - 1)\n",
    "        vp_beta_min = np.log(self.sigma_max ** 2 + 1) - 0.5 * vp_beta_d\n",
    "        sigma = partial(SIGMA_T[self.edm_type], beta_d=vp_beta_d, beta_min=vp_beta_min)\n",
    "        sigma_deriv = partial(SIGMA_T_DERIV[self.edm_type], beta_d=vp_beta_d, beta_min=vp_beta_min)\n",
    "        sigma_inv = partial(SIGMA_T_INV[self.edm_type], beta_d=vp_beta_d, beta_min=vp_beta_min)\n",
    "        scale = partial(SCALE_T[self.edm_type], beta_d=vp_beta_d, beta_min=vp_beta_min)\n",
    "        scale_deriv = partial(SCALE_T_DERIV[self.edm_type], beta_d=vp_beta_d, beta_min=vp_beta_min)\n",
    "\n",
    "        return sigma, sigma_deriv, sigma_inv, scale, scale_deriv\n",
    "  \n",
    "    \n",
    "    def sample(self, \n",
    "               latents: Tensor, \n",
    "               class_labels: Tensor=None, \n",
    "               use_stochastic: bool=False, \n",
    "               **solver_params) -> Tensor:\n",
    "        \n",
    "        # Get sigmas, scales, and timesteps\n",
    "        print(f\"solver_params is {solver_params}\")\n",
    "        num_steps = self.solver_params.num_steps\n",
    "        epsilon_s = self.solver_params.epsilon_s\n",
    "        rho = self.solver_params.rho\n",
    "        \n",
    "        latents = latents.to(self.device)\n",
    "        sigma_steps, t_steps = self._get_sigma_steps_t_steps(num_steps=num_steps, epsilon_s=epsilon_s, rho=rho)\n",
    "        sigma, sigma_deriv, sigma_inv, scale, scale_deriv = self._get_sigma_deriv_inv_scale_deriv()\n",
    "                \n",
    "        S_churn = self.solver_params.S_churn\n",
    "        S_min = self.solver_params.S_min\n",
    "        S_max = self.solver_params.S_max\n",
    "        S_noise = self.solver_params.S_noise\n",
    "        alpha = self.solver_params.alpha\n",
    "        \n",
    "        if not use_stochastic:\n",
    "            # Main sampling loop\n",
    "            t_next = t_steps[0]\n",
    "            x_next = latents.to(torch.float64) * (sigma(t_next) * scale(t_next))\n",
    "            for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n",
    "                x_cur = x_next\n",
    "\n",
    "                # Increase noise temporarily.\n",
    "                gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= sigma(t_cur) <= S_max else 0\n",
    "                t_hat = sigma_inv(self.preconditioner.round_sigma(sigma(t_cur) + gamma * sigma(t_cur)))\n",
    "                x_hat = scale(t_hat) / scale(t_cur) * x_cur + (sigma(t_hat) ** 2 - sigma(t_cur) ** 2).clip(min=0).sqrt() * scale(t_hat) * S_noise * torch.randn_like(x_cur)\n",
    "\n",
    "                # Euler step.\n",
    "                h = t_next - t_hat\n",
    "                denoised = self.preconditioner(x_hat / scale(t_hat), sigma(t_hat), class_labels).to(torch.float64)\n",
    "                d_cur = (sigma_deriv(t_hat) / sigma(t_hat) + scale_deriv(t_hat) / scale(t_hat)) * x_hat - sigma_deriv(t_hat) * scale(t_hat) / sigma(t_hat) * denoised\n",
    "                x_prime = x_hat + alpha * h * d_cur\n",
    "                t_prime = t_hat + alpha * h\n",
    "\n",
    "                # Apply 2nd order correction.\n",
    "                if self.solver_type == 'euler' or i == num_steps - 1:\n",
    "                    x_next = x_hat + h * d_cur\n",
    "                else:\n",
    "                    assert self.solver_type == 'heun'\n",
    "                    denoised = self.preconditioner(x_prime / scale(t_prime), sigma(t_prime), class_labels).to(torch.float64)\n",
    "                    d_prime = (sigma_deriv(t_prime) / sigma(t_prime) + scale_deriv(t_prime) / scale(t_prime)) * x_prime - sigma_deriv(t_prime) * scale(t_prime) / sigma(t_prime) * denoised\n",
    "                    x_next = x_hat + h * ((1 - 1 / (2 * alpha)) * d_cur + 1 / (2 * alpha) * d_prime)\n",
    "        \n",
    "        else:\n",
    "            assert self.edm_type == \"EDM\", f\"Stochastic can only use in EDM, but your precond type is {self.edm_type}\"\n",
    "            x_next = latents.to(torch.float64) * t_steps[0]\n",
    "            for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n",
    "                x_cur = x_next\n",
    "\n",
    "                # Increase noise temporarily.\n",
    "                gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n",
    "                t_hat = self.preconditioner.round_sigma(t_cur + gamma * t_cur)\n",
    "                x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * torch.randn_like(x_cur)\n",
    "\n",
    "                # Euler step.\n",
    "                denoised = self.preconditioner(x_hat, t_hat, class_labels).to(torch.float64)\n",
    "                d_cur = (x_hat - denoised) / t_hat\n",
    "                x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "\n",
    "                # Apply 2nd order correction.\n",
    "                if i < num_steps - 1:\n",
    "                    denoised = self.preconditioner(x_next, t_next, class_labels).to(torch.float64)\n",
    "                    d_prime = (x_next - denoised) / t_next\n",
    "                    x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "\n",
    "\n",
    "        return x_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params is {}\n",
      "solver_params is {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from easydict import EasyDict\n",
    "\n",
    "config = EasyDict(\n",
    "    dict(\n",
    "        device=torch.device(\"cuda\"),  # Test if all tensors are converted to the same device\n",
    "        edm_model=dict(            \n",
    "            path=dict(\n",
    "                edm_type=\"EDM\", # *[\"VP_edm\", \"VE_edm\", \"iDDPM_edm\", \"EDM\"]\n",
    "                params=dict(\n",
    "                    #^ 1: VP_edm\n",
    "                    # beta_d=19.9, \n",
    "                    # beta_min=0.1, \n",
    "                    # M=1000, \n",
    "                    # epsilon_t=1e-5,\n",
    "                    # epsilon_s=1e-3,\n",
    "                    #^ 2: VE_edm\n",
    "                    # sigma_min=0.02,\n",
    "                    # sigma_max=100,\n",
    "                    #^ 3: iDDPM_edm\n",
    "                    # C_1=0.001,\n",
    "                    # C_2=0.008,\n",
    "                    # M=1000,\n",
    "                    #^ 4: EDM\n",
    "                    # sigma_min=0.002,\n",
    "                    # sigma_max=80,\n",
    "                    # sigma_data=0.5,\n",
    "                    # P_mean=-1.2,\n",
    "                    # P_std=1.2,\n",
    "                )\n",
    "            ),\n",
    "            solver=dict(\n",
    "                solver_type=\"heun\", \n",
    "                # *['euler', 'heun']\n",
    "                params=dict(\n",
    "                    num_steps=18,\n",
    "                    alpha=1, \n",
    "                    S_churn=0, \n",
    "                    S_min=0, \n",
    "                    S_max=float(\"inf\"),\n",
    "                    S_noise=1,\n",
    "                    rho=7, #* EDM needs rho \n",
    "                    epsilon_s=1e-3 #* VP_edm needs epsilon_s\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "edm = EDMModel(config).to(config.device)\n",
    "x = torch.randn((1024, 2)).to(config.device)\n",
    "noise = torch.randn_like(x)\n",
    "loss = edm(x).mean()\n",
    "sample = edm.sample(x)\n",
    "sample.shape\n",
    "loss.backward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
